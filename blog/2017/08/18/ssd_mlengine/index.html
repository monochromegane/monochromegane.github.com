<!doctype html><html>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width">
<meta name=author content>
<title>SSD: Single Shot MultiBox DetectorをGoogle Cloud ML Engine上で物体検出APIとして利用する &#183; THINKING MEGANE</title>
<meta name=twitter:card content="summary">
<meta name=twitter:site content="@monochromegane">
<meta property="og:description" content="画像内の物体検出と識別を行う予測APIが必要になったので、物体検出ニューラルネットワークであるSSD: Single Shot MultiBox DetectorをGoogle Cloud ML Engine上で訓練して予測APIとして使えるようにしました。
使い方 Google Cloud ML Engineでモデル、学習時のパラメタ、予測APIのバージョンなどをコード管理するため、StarChartを使います。 コード管理が不要であれば、リポジトリのコードをCloud Storageにアップロードしてgcloudコマンドなどで訓練を行ってください。 訓練データセットなどをCloud Storageに配置した上で、以下のようにして訓練用のジョブを投入します。
$ pip install git+https://github.com/monochromegane/starchart.git $ git clone https://github.com/monochromegane/ssd_mlengine.git $ starchart train -m ssd_mlengine \  -M trainer.task \  -s BASIC_GPU \  -- \  --annotation_path=gs://PATH_TO_ANNOTATION_FILE \  --prior_path=gs://PATH_TO_PRIOR_FILE \  --weight_path=gs://PATH_TO_WEIGHT_FILE \  --images_path=gs://PATH_TO_IMAGES_FILE \  --model_dir=TRAIN_PATH/model 訓練が終わったら、以下のようにしてモデルを予測APIとして公開します。
$ starchart expose -m ssd_mlengine 公開された予測APIを叩くと結果が返ります。
$ python predict.py -k 1 -c 0.45 -i image.">
<meta property="og:title" content="SSD: Single Shot MultiBox DetectorをGoogle Cloud ML Engine上で物体検出APIとして利用する">
<meta property="og:url" content="https://blog.monochromegane.com/blog/2017/08/18/ssd_mlengine/">
<meta property="og:image" content="https://blog.monochromegane.com/images/ogp.png">
<link rel=stylesheet href=/css/reset.css>
<link rel=stylesheet href="//fonts.googleapis.com/css?family=Source+Code+Pro" type=text/css>
<link rel=stylesheet href="//fonts.googleapis.com/css?family=Montserrat" type=text/css>
<link rel=stylesheet href=//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css>
<link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/ir_black.min.css>
<link rel=stylesheet href=/css/text.css>
<link rel=stylesheet href=/css/color.css>
<link rel=stylesheet href=/css/layout.css>
<link rel=apple-touch-icon-precomposed sizes=57x57 href=/images/apple-touch-icon-57-precomposed.png>
<link rel=apple-touch-icon-precomposed sizes=114x114 href=/images/apple-touch-icon-114-precomposed.png>
<link rel=apple-touch-icon-precomposed sizes=72x72 href=/images/apple-touch-icon-72-precomposed.png>
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/images/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/images/favicon.ico>
<link rel=canonical href=https://blog.monochromegane.com/blog/2017/08/18/ssd_mlengine/>
<link href rel=alternate type=application/rss+xml title="THINKING MEGANE">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML" async>MathJax.Hub.Config({HTML:["input/TeX","output/HTML-CSS"],TeX:{Macros:{bm:["\\boldsymbol{#1}",1],argmax:["\\mathop{\\rm arg\\,max}\\limits"],argmin:["\\mathop{\\rm arg\\,min}\\limits"]},extensions:["AMSmath.js","AMSsymbols.js"],equationNumbers:{autoNumber:"AMS"}},extensions:["tex2jax.js"],jax:["input/TeX","output/HTML-CSS"],tex2jax:{inlineMath:[['$','$'],["\\(","\\)"]],displayMath:[['$$','$$'],["\\[","\\]"]],processEscapes:!0},"HTML-CSS":{availableFonts:["TeX"],linebreaks:{automatic:!0}}})</script>
</head>
<body>
<div class=header>
<header>
<h1 class=logo><a href=https://blog.monochromegane.com>THINKING MEGANE</a></h1>
<nav>
<ul class=menu>
<li><a href=/self-introduction/>Home</a></li>
<li><a href=/post/>Archives</a></li>
<li><a href=/diary/>Diary</a></li>
<li><a href=http://thinking-megane.blogspot.jp/>Old Blog</a></li>
</ul>
</nav>
</header>
</div>
<div class=container>
<article>
<div class=post>
<header>
<h1><a href=https://blog.monochromegane.com/blog/2017/08/18/ssd_mlengine/>SSD: Single Shot MultiBox DetectorをGoogle Cloud ML Engine上で物体検出APIとして利用する</a></h1>
<time><div class=day><i class="fa fa-calendar-o day-icon"></i>2017-08-18</div></time>
</header>
<div class=post-inner>
<p>画像内の物体検出と識別を行う予測APIが必要になったので、物体検出ニューラルネットワークである<a href=http://arxiv.org/abs/1512.02325>SSD: Single Shot MultiBox Detector</a>を<a href=https://cloud.google.com/products/machine-learning/>Google Cloud ML Engine</a>上で訓練して予測APIとして使えるようにしました。</p>
<h2 id=使い方>使い方</h2>
<p>Google Cloud ML Engineでモデル、学習時のパラメタ、予測APIのバージョンなどをコード管理するため、<a href=https://github.com/monochromegane/starchart>StarChart</a>を使います。
コード管理が不要であれば、リポジトリのコードをCloud Storageにアップロードして<code>gcloud</code>コマンドなどで訓練を行ってください。
訓練データセットなどをCloud Storageに配置した上で、以下のようにして訓練用のジョブを投入します。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>$ pip install git+https://github.com/monochromegane/starchart.git
$ git clone https://github.com/monochromegane/ssd_mlengine.git
$ starchart train -m ssd_mlengine <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  -M trainer.task <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  -s BASIC_GPU <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  -- <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  --annotation_path<span style=color:#f92672>=</span>gs://PATH_TO_ANNOTATION_FILE <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  --prior_path<span style=color:#f92672>=</span>gs://PATH_TO_PRIOR_FILE <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  --weight_path<span style=color:#f92672>=</span>gs://PATH_TO_WEIGHT_FILE <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  --images_path<span style=color:#f92672>=</span>gs://PATH_TO_IMAGES_FILE <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>                  --model_dir<span style=color:#f92672>=</span>TRAIN_PATH/model
</code></pre></div><p>訓練が終わったら、以下のようにしてモデルを予測APIとして公開します。</p>
<pre tabindex=0><code>$ starchart expose -m ssd_mlengine
</code></pre><p>公開された予測APIを叩くと結果が返ります。</p>
<pre tabindex=0><code>$ python predict.py -k 1 -c 0.45 -i image.jpg
# {'predictions': [{'key': '1',
#    'objects': [[8.0,        # 検出した物体の分類区分
#      0.45196059346199036,   # 確率
#      104.90727233886719,    # 検出した位置(xmin)
#      97.99836730957031,     # 検出した位置(ymin)
#      212.12222290039062,    # 検出した位置(xmax)
#      315.3045349121094]]}]} # 検出した位置(ymax)
</code></pre><p>予測APIを叩くコードはREADMEを参考にしてください。パラメタは以下の通りです。</p>
<ul>
<li>keep_top_k(-k): 確率の降順でソートされたうち、上位何件を取得するか</li>
<li>confidence_threshold(-c): 確率の閾値</li>
</ul>
<h2 id=実装>実装</h2>
<p>SSDのKeras実装である<a href=https://github.com/rykov8/ssd_keras>rykov8/ssd_keras</a>を参考にGoogle Cloud ML Engineで訓練、予測APIとして利用できるようにしています。</p>
<p>SSDはモデルの出力をそのまま画像内の検出した位置として利用できないため、元画像で利用できるよう結果を変換する必要がありますが、ちと面倒なのでこの部分もAPI側で行うようにします。
このような予測APIだけ必要となる変換層はKerasとML Engineを利用する場合、以下のようにして追加できます。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># keras.layers.Lambdaで学習モデルの出力を入力として変換する層を定義</span>
detection_out <span style=color:#f92672>=</span> Lambda(bbox_util<span style=color:#f92672>.</span>detection_out, arguments<span style=color:#f92672>=</span>{
    <span style=color:#e6db74>&#39;keep_top_k&#39;</span>: keep_top_k_placeholder,
    <span style=color:#e6db74>&#39;confidence_threshold&#39;</span>: confidence_threshold_placeholder,
    <span style=color:#e6db74>&#39;original_size&#39;</span>: original_size_placeholder
    })(net[<span style=color:#e6db74>&#39;predictions&#39;</span>])

<span style=color:#75715e># 上で定義したLambdaを予測APIの出力として定義</span>
inputs  <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;key&#39;</span>: keys_placeholder,
           <span style=color:#e6db74>&#39;data&#39;</span>: model<span style=color:#f92672>.</span>input,
           <span style=color:#e6db74>&#39;keep_top_k&#39;</span>: keep_top_k_placeholder,
           <span style=color:#e6db74>&#39;confidence_threshold&#39;</span>: confidence_threshold_placeholder,
           <span style=color:#e6db74>&#39;original_size&#39;</span>: original_size_placeholder}
outputs <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;key&#39;</span>: tf<span style=color:#f92672>.</span>identity(keys_placeholder), <span style=color:#e6db74>&#39;objects&#39;</span>: detection_out}
</code></pre></div><p>これらの定義をSavedModel形式で保存することでML Engineが予測APIとして扱えるようにします</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_signature</span>(inputs, outputs):
    signature_inputs <span style=color:#f92672>=</span> {key: saved_model_utils<span style=color:#f92672>.</span>build_tensor_info(tensor)
                        <span style=color:#66d9ef>for</span> key, tensor <span style=color:#f92672>in</span> inputs<span style=color:#f92672>.</span>items()}
    signature_outputs <span style=color:#f92672>=</span> {key: saved_model_utils<span style=color:#f92672>.</span>build_tensor_info(tensor)
                         <span style=color:#66d9ef>for</span> key, tensor <span style=color:#f92672>in</span> outputs<span style=color:#f92672>.</span>items()}

    signature_def <span style=color:#f92672>=</span> signature_def_utils<span style=color:#f92672>.</span>build_signature_def(
        signature_inputs, signature_outputs,
        signature_constants<span style=color:#f92672>.</span>PREDICT_METHOD_NAME)

    <span style=color:#66d9ef>return</span> signature_def

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>export</span>(sess, inputs, outputs, output_dir):
    <span style=color:#66d9ef>if</span> file_io<span style=color:#f92672>.</span>file_exists(output_dir):
        file_io<span style=color:#f92672>.</span>delete_recursively(output_dir)

    signature_def <span style=color:#f92672>=</span> build_signature(inputs, outputs)

    signature_def_map <span style=color:#f92672>=</span> {
        signature_constants<span style=color:#f92672>.</span>DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def
    }
    builder <span style=color:#f92672>=</span> saved_model_builder<span style=color:#f92672>.</span>SavedModelBuilder(output_dir)
    builder<span style=color:#f92672>.</span>add_meta_graph_and_variables(
        sess,
        tags<span style=color:#f92672>=</span>[tag_constants<span style=color:#f92672>.</span>SERVING],
        signature_def_map<span style=color:#f92672>=</span>signature_def_map)

    builder<span style=color:#f92672>.</span>save()

<span style=color:#75715e># SavedModel形式で保存する</span>
export(get_session(), inputs, outputs, FLAGS<span style=color:#f92672>.</span>model_dir)
</code></pre></div><p>なお、今回は、ML Engineの保存ファイルが256MB以内とする制限を回避するため、ベースネットワークのconv4~pool4層も固定し、その出力を元に38x38分割したサイズの物体を検出するレイヤも除去することで学習パラメタを削減しています。論文では出力レイヤを減らすと精度が低下するとあるので、このあたりは対象のドメインや要求する精度によって調整が必要かもしれません。</p>
<h2 id=学習に必要となるデータなど>学習に必要となるデータなど</h2>
<p>学習済みの重みやバイアスを保存した <code>weights_SSD300.hdf5</code> と、計算済みのデフォルトボックスの位置を保存した <code>prior_boxes_ssd300.pkl</code> を <a href=https://github.com/rykov8/ssd_keras>rykov8/ssd_keras</a>から入手しておきます。</p>
<p>前述の通り、デフォルトボックスを除去しているので、</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>import</span> pickle
pickle<span style=color:#f92672>.</span>load(open(<span style=color:#e6db74>&#39;prior_boxes_ssd300.pkl&#39;</span>, <span style=color:#e6db74>&#39;rb&#39;</span>))[<span style=color:#ae81ff>4332</span>:]<span style=color:#f92672>.</span>dump(<span style=color:#e6db74>&#39;prior_boxes_ssd300_min.pkl&#39;</span>)
</code></pre></div><p>のようにして、必要なものだけを取り出しておく必要があります。</p>
<p>また、教師データとして、画像ならびにその画像に対する物体の位置と分類クラスを定義したデータセットが必要です。今回はrykov8/ssd_kerasと同じくVOCのデータセット形式を採用しています。
独自にデータセットを準備できたら、<a href=https://github.com/rykov8/ssd_keras/blob/master/PASCAL_VOC/get_data_from_XML.py>rykov8/ssd_kerasの提供しているツール</a>を使ってpickle形式で保存したものをannotation_pathとして、もととなった画像データをtar.gzで固めたものをimages_pathとして指定します。</p>
<h2 id=まとめ>まとめ</h2>
<p>物体検出ニューラルネットワークであるSSDのKeras実装をGoogle Cloud ML Engine上で訓練し、予測APIとして提供できるようにしました。実際にAPIとして運用するにあたってはモデル、学習時のパラメタ、予測APIのバージョンなどをコード管理が求められることになるため、<a href=https://github.com/monochromegane/starchart>StarChart</a>のようなツールを検討も大切だと思います。StarChartについては<a href=https://rand.pepabo.com/article/2017/01/18/pepabo-ml-platform-and-workflow/>こちら</a>や<a href=https://speakerdeck.com/monochromegane/pepabo-ml-infrastructure-starchart>こちら</a>でも紹介しています。よければ参考にしてください。</p>
</div>
<footer class=tag>
<i class="fa fa-tags tag-icon"></i>
<ul>
<li><a class=tag-link href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92>機械学習</a></li>
</ul>
<div class=clear></div>
</footer>
</div>
</article>
<div class=social-btn>
<div>
<a href=http://b.hatena.ne.jp/entry/s/blog.monochromegane.com/blog/2017/08/18/ssd_mlengine/ class=hatena-bookmark-button data-hatena-bookmark-layout=basic-label-counter data-hatena-bookmark-lang=en title=このエントリーをはてなブックマークに追加><img src=https://b.st-hatena.com/images/entry-button/button-only@2x.png alt=このエントリーをはてなブックマークに追加 width=20 height=20 style=border:none></a><script type=text/javascript src=https://b.st-hatena.com/js/bookmark_button.js async></script>
</div>
<div>
<a href=https://twitter.com/share class=twitter-share-button data-via=monochromegane>Tweet</a>
<script>!function(a,c,d){var b,e=a.getElementsByTagName(c)[0],f=/^http:/.test(a.location)?'http':'https';a.getElementById(d)||(b=a.createElement(c),b.id=d,b.src=f+'://platform.twitter.com/widgets.js',e.parentNode.insertBefore(b,e))}(document,'script','twitter-wjs')</script>
</div>
</div>
</div>
<footer class=footer>
<div class=author></div>
<p class=author-name><a href=https://twitter.com/monochromegane target=_blank>@monochromegane</a></p>
<p>monochromegane (モノクロめがね) と申します。</p>
<p>最近はもっぱらGoに夢中です。普段は研究開発をやっています。</p>
<ul class=social>
<li><a href=https://twitter.com/monochromegane target=_blank><i class="fa fa-twitter"></i></a></li>
<li><a href=https://github.com/monochromegane target=_blank><i class="fa fa-github"></i></a></li>
<li><a href=/index.xml target=_blank><i class="fa fa-rss"></i></a></li>
</ul>
<p class=copyright>
<small>&copy; <span>Powered by <a href=http://gohugo.io target=_blank>Hugo</a>, Designed by <a href=https://twitter.com/keita_kawamoto target=_blank>&copy;keita_kawamoto</a></span></small>
</p>
</footer>
<script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js></script>
<script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/languages/go.min.js></script>
<script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/languages/vim.min.js></script>
<script>hljs.initHighlightingOnLoad()</script>
<script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-38374674-1','auto'),ga('send','pageview')</script>
</body>
</html>